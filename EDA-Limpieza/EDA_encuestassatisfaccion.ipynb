{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. EDA\n",
    "En este apartado se pretende analizar los datasets para poder enfocar mejor la limpieza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Importación y carga de datos\n",
    "Debemos declarar las librerías que usamos y leer el correspondiente archivo de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de librerías\n",
    "import pandas as pd\n",
    "\n",
    "# Lectura dataset\n",
    "df = pd.read_csv('../EncuestasSatisfaccionSucio.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Configuración de Pandas\n",
    "Para poder leer bien los resultados de las ejecuciones, vamos a configurar tanto el número máximo de columnas como el número máximo de filas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número máximo de filas a mostrar\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Número máximo de columnas a mostrar\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Descripción general del dataset\n",
    "Para poder conocer ciertas características relevantes del dataset, como el número de instancias (filas) y características (columnas) procederemos a usar diferentes funciones de Pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                ID  PUNTUACION_ACCESIBILIDAD  PUNTUACION_CALIDAD  AreaRecreativaID\n",
      "count  7000.000000               7000.000000         7000.000000      7.000000e+03\n",
      "mean   3500.500000                  2.976857            2.995286      1.545206e+06\n",
      "std    2020.870275                  1.416548            1.399429      2.802642e+06\n",
      "min       1.000000                  1.000000            1.000000      1.665700e+04\n",
      "25%    1750.750000                  2.000000            2.000000      1.742200e+04\n",
      "50%    3500.500000                  3.000000            3.000000      1.811450e+04\n",
      "75%    5250.250000                  4.000000            4.000000      4.515310e+05\n",
      "max    7000.000000                  5.000000            5.000000      9.503906e+06\n",
      "\n",
      "\n",
      "Número de filas:  7000\n",
      "Número de columnas:  6\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7000 entries, 0 to 6999\n",
      "Data columns (total 6 columns):\n",
      " #   Column                    Non-Null Count  Dtype \n",
      "---  ------                    --------------  ----- \n",
      " 0   ID                        7000 non-null   int64 \n",
      " 1   PUNTUACION_ACCESIBILIDAD  7000 non-null   int64 \n",
      " 2   PUNTUACION_CALIDAD        7000 non-null   int64 \n",
      " 3   COMENTARIOS               7000 non-null   object\n",
      " 4   AreaRecreativaID          7000 non-null   int64 \n",
      " 5   FECHA                     7000 non-null   object\n",
      "dtypes: int64(4), object(2)\n",
      "memory usage: 328.3+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Descripción de parámetros generales -> count, mean, std, min, 25%, 50%, 75%, max\n",
    "print(df.describe())\n",
    "\n",
    "# Número de filas y columnas del dataset\n",
    "print(\"\\n\")\n",
    "print(\"Número de filas: \", df.shape[0])\n",
    "print(\"Número de columnas: \", df.shape[1])\n",
    "print(\"\\n\")\n",
    "\n",
    "# Para saber el tipo de variable de cada columna\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Observación inicial del dataset\n",
    "Vamos a mostrar 30 entradas para poder observar cómo es realmente por dentro el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ----------    10 primeras filas    ----------    \n",
      "\n",
      "\n",
      "   ID  PUNTUACION_ACCESIBILIDAD  PUNTUACION_CALIDAD COMENTARIOS  AreaRecreativaID       FECHA\n",
      "0   1                         2                   1     Regular             18510  27-03-2021\n",
      "1   2                         5                   5   Excelente             52143  25/03/2023\n",
      "2   3                         1                   1  Deficiente           6907760  19-03-2022\n",
      "3   4                         2                   1     Regular           9187084  2021-06-25\n",
      "4   5                         3                   2   Aceptable             17256  28-12-2023\n",
      "5   6                         5                   5   Excelente             18570  2021-12-09\n",
      "6   7                         1                   1  Deficiente             17647  2021/02/22\n",
      "7   8                         1                   2     Regular             52103  12-15-2022\n",
      "8   9                         1                   1  Deficiente             52195  2021/10/07\n",
      "9  10                         2                   2   Aceptable             52223  05-24-2023\n",
      "\n",
      "\n",
      "   ----------    10 filas aleatorias    ----------    \n",
      "\n",
      "\n",
      "        ID  PUNTUACION_ACCESIBILIDAD  PUNTUACION_CALIDAD COMENTARIOS  AreaRecreativaID       FECHA\n",
      "3702  3703                         2                   2   Aceptable             52154  2021/06/28\n",
      "5779  5780                         2                   1     Regular             17455  2022/06/21\n",
      "2001  2002                         3                   2   Aceptable             18133  25/06/2022\n",
      "3107  3108                         2                   3   Aceptable             52040  2023-02-24\n",
      "938    939                         2                   2   Aceptable           9261936  02/01/2022\n",
      "6091  6092                         3                   4   Muy bueno             18302  16-11-2022\n",
      "6382  6383                         1                   1  Deficiente             17732  15/10/2021\n",
      "3184  3185                         3                   2   Aceptable             16770  05/06/2021\n",
      "2047  2048                         4                   3   Muy bueno             18491  24/04/2021\n",
      "5030  5031                         2                   2   Aceptable             17884  09-05-2023\n",
      "\n",
      "\n",
      "   ----------    10 últimas filas    ----------    \n",
      "\n",
      "\n",
      "        ID  PUNTUACION_ACCESIBILIDAD  PUNTUACION_CALIDAD COMENTARIOS  AreaRecreativaID       FECHA\n",
      "6990  6991                         5                   4   Excelente             17694  06/11/2022\n",
      "6991  6992                         3                   4   Muy bueno             17411  2023/03/27\n",
      "6992  6993                         5                   5   Excelente             16941  2022-01-25\n",
      "6993  6994                         3                   3   Muy bueno             17575  23-08-2023\n",
      "6994  6995                         5                   5   Excelente             18036  2021-01-31\n",
      "6995  6996                         4                   5   Excelente             17047  10-06-2022\n",
      "6996  6997                         4                   4   Excelente             18227  03-29-2023\n",
      "6997  6998                         4                   5   Excelente             17637  08-06-2021\n",
      "6998  6999                         5                   5   Excelente             16678  2023-02-08\n",
      "6999  7000                         1                   2     Regular           7143517  04-03-2023\n"
     ]
    }
   ],
   "source": [
    "# Mostramos las 10 primeras filas, 10 filas aleatorias y las 10 últimas\n",
    "print(\"   ----------    10 primeras filas    ----------    \")\n",
    "print(\"\\n\")\n",
    "print(df.head(10))\n",
    "print(\"\\n\")\n",
    "print(\"   ----------    10 filas aleatorias    ----------    \")\n",
    "print(\"\\n\")\n",
    "print(df.sample(10))\n",
    "print(\"\\n\")\n",
    "print(\"   ----------    10 últimas filas    ----------    \")\n",
    "print(\"\\n\")\n",
    "print(df.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Revisión de valores nulos\n",
    "Como ya se ha visto en el anterior apartado (info), podemos observar los valores nulos de esa forma. Pero se puede observar de una forma más visual en esta sección y, además, hay que tener en cuenta valores como el cero que también pueden considerarse nulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ----------    Valores faltantes    ----------    \n",
      "\n",
      "\n",
      "ID                          0\n",
      "PUNTUACION_ACCESIBILIDAD    0\n",
      "PUNTUACION_CALIDAD          0\n",
      "COMENTARIOS                 0\n",
      "AreaRecreativaID            0\n",
      "FECHA                       0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "    ----------    Valores cero    ----------    \n",
      "\n",
      "\n",
      "ID                          0\n",
      "PUNTUACION_ACCESIBILIDAD    0\n",
      "PUNTUACION_CALIDAD          0\n",
      "COMENTARIOS                 0\n",
      "AreaRecreativaID            0\n",
      "FECHA                       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Para poder saber el número de valores faltantes\n",
    "print(\"    ----------    Valores faltantes    ----------    \")\n",
    "print(\"\\n\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Para poder saber el número de ceros en cada columna\n",
    "print(\"    ----------    Valores cero    ----------    \")\n",
    "print(\"\\n\")\n",
    "print((df == 0).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No encontramos valores nulos, por lo que no se hará limpieza de esto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Identificación de fechas no estandarizadas\n",
    "Se deben identificar las fechas que no se encuentran en el formato adecuado para MongoDB (DD/MM/YYYY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ----------    Fechas    ----------    \n",
      "\n",
      "\n",
      "6679    2021-03-09\n",
      "3995    24-10-2023\n",
      "4160    09/04/2023\n",
      "5249    2023-10-03\n",
      "2543    2021/08/03\n",
      "1924    2022/04/28\n",
      "5678    11-24-2023\n",
      "1577    12-18-2021\n",
      "3433    16/12/2023\n",
      "4428    2021/07/26\n",
      "2712    2021-03-29\n",
      "2105    08-20-2022\n",
      "5690    26-05-2023\n",
      "2753    2021/09/02\n",
      "321     01-19-2021\n",
      "959     25/03/2021\n",
      "6362    2021-04-04\n",
      "99      01-14-2021\n",
      "4721    17/06/2023\n",
      "2547    07-10-2021\n",
      "Name: FECHA, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Vamos a mostrar algunas fechas para poder observar en qué formato están\n",
    "print(\"   ----------    Fechas    ----------    \")\n",
    "print(\"\\n\")\n",
    "print(df['FECHA'].sample(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede apreciar, las fechas se encuentran en diversos formatos que deben ser homogeneizados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Identificación de registros duplicados\n",
    "Debemos validar que no existen filas iguales que ensucien el dataset, sobre todo estando pendiente de duplicaciones de la clave primaria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ----------    Filas duplicadas    ----------    \n",
      "\n",
      "\n",
      "Empty DataFrame\n",
      "Columns: [ID, PUNTUACION_ACCESIBILIDAD, PUNTUACION_CALIDAD, COMENTARIOS, AreaRecreativaID, FECHA]\n",
      "Index: []\n",
      "\n",
      "\n",
      "Número de filas duplicadas:  0\n",
      "\n",
      "\n",
      "   ----------    Filas con el mismo ID    ----------    \n",
      "\n",
      "\n",
      "Empty DataFrame\n",
      "Columns: [ID, PUNTUACION_ACCESIBILIDAD, PUNTUACION_CALIDAD, COMENTARIOS, AreaRecreativaID, FECHA]\n",
      "Index: []\n",
      "\n",
      "\n",
      "Número de filas con el mismo ID:  0\n"
     ]
    }
   ],
   "source": [
    "# Ver las filas duplicadas\n",
    "print(\"   ----------    Filas duplicadas    ----------    \")\n",
    "print(\"\\n\")\n",
    "print(df[df.duplicated()])\n",
    "print(\"\\n\")\n",
    "\n",
    "# Número de filas duplicadas\n",
    "print(\"Número de filas duplicadas: \", df.duplicated().sum())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Filtrar filas que tienen el mismo ID\n",
    "duplicados = df.groupby('ID').filter(lambda x: len(x) > 1)\n",
    "\n",
    "# Ordenar por NIF para que las filas con el mismo NIF se visualicen una encima de la otra\n",
    "duplicados = duplicados.sort_values(by='ID')\n",
    "\n",
    "# Mostrar las filas con la misma PK\n",
    "print(\"   ----------    Filas con el mismo ID    ----------    \")\n",
    "print(\"\\n\")\n",
    "print(duplicados)\n",
    "print(\"\\n\")\n",
    "print(\"Número de filas con el mismo ID: \", duplicados.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No existen filas duplicadas, por lo que no es necesario limpiar en este paso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 Búsqueda de errores tipográficos\n",
    "Hay ciertos atributos de texto que pueden contar con determinados errores tipográficos que deben ser solucionados, como los nombres de áreas, juegos, usuarios y ubicaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este dataset no encontramos ningún atributo similar, por lo que este paso no se realizará"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.9 Identificación de valores enum fuera de campo\n",
    "Hay ciertos atributos que solo deben poseer ciertos valores (como Operativo-NoOperativo). Hace falta identificar aquellos valores de ese campo fuera de la norma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No hay atributos Enum, por lo que este paso se saltará"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.10 Validación de las coordenadas y otros campos geoespaciales\n",
    "Hay algunas veces en las que los códigos postales no respetan la identificación de Madrid (280..) o el formato, ya sean códigos postales u otros atributos de geolocalización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este dataset no encontramos valores de localización, por lo que este paso no se realizará"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.11 Identificación de unidades de medida en un formato no estandarizado\n",
    "Se deben identificar las filas que no posean un formato estándar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este dataset no encontramos valores de unidades de medida, por lo que este paso no se realizará"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.12 Otros atributos a corregir\n",
    "En esta sección se mencionarán aquellos atributos que también deban ser limpiados por errores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.12.1 Validación de IDs\n",
    "Para este paso debemos comprobar que todos los IDs son enteros, ya que en el paso de los duplicados ya hemos comprobado esto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ----------    Validación de IDs    ----------    \n",
      "\n",
      "\n",
      "El ID es un entero:  True\n"
     ]
    }
   ],
   "source": [
    "# Debemos validar que todos los IDs son únicos y enteros\n",
    "print(\"   ----------    Validación de IDs    ----------    \")\n",
    "print(\"\\n\")\n",
    "print('El ID es un entero: ', df['ID'].apply(lambda x: x.is_integer()).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todos los IDs son enteros, por lo que no se precisará su limpieza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.12.2 Consideraciones extras de Fechas\n",
    "Cabe recalcar que si el campo de fechas no posee un formato esperado, las funciones de limpieza darán error, por lo que no hace falta hacer ahora las comprobaciones de sus valores para determinar si son correctos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.12.3 Validación de campos con formato Enum\n",
    "Aunque no sean Enum como tal, hay columnas que se pueden validar siguiendo un proceso parecido al que usamos al tener Enums. En concreto, la columna de 'PUNTUACION_ACCESIBILIDAD', 'PUNTUACION_CALIDAD' Y 'COMENTARIOS' se puede validar viendo sus posibles valores y comprobando si tienen sentido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ----------    Valores distintos de la columna 'PUNTUACION_ACCESIBILIDAD'    ----------    \n",
      "\n",
      "\n",
      "[2 5 1 3 4]\n",
      "\n",
      "\n",
      "   ----------    Valores distintos de la columna 'PUNTUACION_CALIDAD'    ----------    \n",
      "\n",
      "\n",
      "[1 5 2 4 3]\n",
      "\n",
      "\n",
      "   ----------    Valores distintos de la columna 'COMENTARIOS'    ----------    \n",
      "\n",
      "\n",
      "['Regular' 'Excelente' 'Deficiente' 'Aceptable' 'Muy bueno']\n"
     ]
    }
   ],
   "source": [
    "# Muestra todos los valores distintos de la columna 'PUNTUACION_ACCESIBILIDAD'\n",
    "print(\"   ----------    Valores distintos de la columna 'PUNTUACION_ACCESIBILIDAD'    ----------    \")\n",
    "print(\"\\n\")\n",
    "print(df['PUNTUACION_ACCESIBILIDAD'].unique())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Muestra todos los valores distintos de la columna 'PUNTUACION_CALIDAD'\n",
    "print(\"   ----------    Valores distintos de la columna 'PUNTUACION_CALIDAD'    ----------    \")\n",
    "print(\"\\n\")\n",
    "print(df['PUNTUACION_CALIDAD'].unique())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Muestra todos los valores distintos de la columna 'COMENTARIOS'\n",
    "print(\"   ----------    Valores distintos de la columna 'COMENTARIOS'    ----------    \")\n",
    "print(\"\\n\")\n",
    "print(df['COMENTARIOS'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como poseen valores que tienen sentido, no hace falta tener que limpiarlo al no tener nulos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.12.4 Consideraciones extras de AreaRecreativaID\n",
    "Se ha observado que los IDs presentes poseen un formato idéntico a los IDs del dataset de áreas (como debe ser), así que debemos validar que todos los valores de esta columna se encuentran allí para confirmar que se pueden enlazar ambos datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ----------    Comprobación de valores en 'AreaRecreativaID'    ----------    \n",
      "\n",
      "\n",
      "Todos los valores se encuentran en el otro dataset:  True\n"
     ]
    }
   ],
   "source": [
    "# Comprobar que todos los valores de la columna 'AreaRecreativaID' existen en la columna 'ID' del dataset 'AreasSucio.csv'\n",
    "areas = pd.read_csv('../AreasSucio.csv')\n",
    "print(\"   ----------    Comprobación de valores en 'AreaRecreativaID'    ----------    \")\n",
    "print(\"\\n\")\n",
    "print(\"Todos los valores se encuentran en el otro dataset: \", df['AreaRecreativaID'].isin(areas['ID']).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como todos los valores se encuentran en el otro dataset, no hace falta hacer transformaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Limpieza de los datasets\n",
    "En este apartado se realizará la limpieza según la información obtenida en el análisis exploratorio de datos:\n",
    "- Se debe corregir las fechas y dejarlas en un formato homogéneo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Limpieza de FECHA\n",
    "Como en otros datasets, debemos dejar todas las fechas en formato apto para MongoDB (DD/MM/YYYY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ----------    Columna de fechas corregidas    ----------    \n",
      "\n",
      "\n",
      "1893    29/03/2021\n",
      "243     07/07/2023\n",
      "961     23/11/2022\n",
      "4067    17/06/2022\n",
      "5617    15/06/2021\n",
      "615     19/01/2022\n",
      "332     05/09/2023\n",
      "6116    28/10/2023\n",
      "3224    09/02/2021\n",
      "2619    14/11/2022\n",
      "621     21/03/2022\n",
      "3741    25/12/2022\n",
      "2385    16/11/2023\n",
      "1928    16/10/2022\n",
      "3817    27/02/2022\n",
      "2919    19/08/2022\n",
      "4670    06/06/2023\n",
      "4020    24/01/2023\n",
      "1227    26/02/2021\n",
      "5117    13/03/2022\n",
      "Name: FECHA, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Leemos toda la columna de fechas y almacenamos todas las fechas en una lista\n",
    "fechas = df['FECHA'].tolist()\n",
    "\n",
    "# Ahora, pasamos por todas las fechas y corregimos las fechas que están mal escritas\n",
    "fechas_corregidas = []\n",
    "año_al_final = False\n",
    "\n",
    "for fecha in fechas:\n",
    "    # Si fecha contiene un guion, hacer fecha.split('-') y si fecha contiene una barra, hacer fecha.split('/')\n",
    "    if '-' in fecha:\n",
    "        fecha_split = fecha.split('-')\n",
    "    elif '/' in fecha:\n",
    "        fecha_split = fecha.split('/')\n",
    "    # Condiciones para saber el día, mes y año\n",
    "    # Encontrar el año\n",
    "    if (int(fecha_split[0])>31):\n",
    "        año = fecha_split[0]\n",
    "        año_al_final = False\n",
    "    elif (int(fecha_split[2])>31):\n",
    "        año = fecha_split[2]\n",
    "        año_al_final = True\n",
    "    else:\n",
    "        print(\"No se ha encontrado el año\")\n",
    "        fechas_corregidas.append(\"fecha incorrecta\")\n",
    "        break\n",
    "    # Encontrar el día y el mes\n",
    "    # Si el año está al final, no comprobamos el primer caracter\n",
    "    if año_al_final:\n",
    "        if (int(fecha_split[0])<32 and int(fecha_split[0])>12):\n",
    "            dia = fecha_split[0]\n",
    "            mes = fecha_split[1]\n",
    "        elif (int(fecha_split[1])<32 and int(fecha_split[1])>12):\n",
    "            dia = fecha_split[1]\n",
    "            mes = fecha_split[0]\n",
    "        # Si no hay ningun número entre el 13 y el 31, se asume que el mes es el segundo siempre y el día es el primero\n",
    "        else:\n",
    "            dia = fecha_split[0]\n",
    "            mes = fecha_split[1]\n",
    "    else:\n",
    "        if (int(fecha_split[1])<32 and int(fecha_split[1])>12):\n",
    "            dia = fecha_split[1]\n",
    "            mes = fecha_split[2]\n",
    "        elif (int(fecha_split[2])<32 and int(fecha_split[2])>12):\n",
    "            dia = fecha_split[2]\n",
    "            mes = fecha_split[1]\n",
    "        # Si no hay ningun número entre el 13 y el 31, se asume que el mes es el segundo siempre y el día es el tercero\n",
    "        else:\n",
    "            dia = fecha_split[2]\n",
    "            mes = fecha_split[1]\n",
    "    fechas_corregidas.append(dia+\"/\"+mes+\"/\"+año)\n",
    "    \n",
    "# Ahora, cambiamos toda la columna de FECHAS_INTERVENCION por las fechas corregidas\n",
    "df['FECHA'] = fechas_corregidas\n",
    "\n",
    "# Mostramos las fechas corregidas\n",
    "print(\"   ----------    Columna de fechas corregidas    ----------    \")\n",
    "print(\"\\n\")\n",
    "print(df['FECHA'].sample(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Mostrar Dataset Limpio y guardar CSV\n",
    "Vamos a mostrar algunas filas del dataset limpio para validar que todo está OK y guardamos el Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ----------    10 filas aleatorias    ----------    \n",
      "\n",
      "\n",
      "        ID  PUNTUACION_ACCESIBILIDAD  PUNTUACION_CALIDAD COMENTARIOS  AreaRecreativaID       FECHA\n",
      "6434  6435                         3                   4   Muy bueno             17795  26/06/2021\n",
      "2520  2521                         3                   2   Aceptable             18047  03/11/2021\n",
      "1390  1391                         1                   1  Deficiente             17488  27/01/2023\n",
      "2172  2173                         4                   5   Excelente             17301  07/09/2022\n",
      "5068  5069                         4                   5   Excelente             52089  26/10/2022\n",
      "4466  4467                         3                   2   Aceptable             17470  02/04/2021\n",
      "4880  4881                         5                   4   Excelente           8799606  02/01/2023\n",
      "1579  1580                         4                   5   Excelente             52200  14/02/2022\n",
      "2197  2198                         4                   4   Excelente             17416  24/07/2022\n",
      "4001  4002                         4                   3   Muy bueno             52040  02/11/2023\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dataset limpio guardado\n"
     ]
    }
   ],
   "source": [
    "# Enseñamos 20 filas aleatorias para ver cómo quedan\n",
    "print(\"   ----------    10 filas aleatorias    ----------    \")\n",
    "print(\"\\n\")\n",
    "print(df.sample(10))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Guardamos el dataset limpio\n",
    "df.to_csv('../EncuestasSatisfaccionLimpio.csv', index=False)\n",
    "print(\"\\n\")\n",
    "print(\"Dataset limpio guardado\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

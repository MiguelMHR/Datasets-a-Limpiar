{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. EDA\n",
    "En este apartado se pretende analizar los datasets para poder enfocar mejor la limpieza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Importación y carga de datos\n",
    "Debemos declarar las librerías que usamos y leer el correspondiente archivo de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de librerías\n",
    "import pandas as pd\n",
    "\n",
    "# Lectura dataset\n",
    "df = pd.read_csv('../IncidentesSeguridadSucio.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Configuración de Pandas\n",
    "Para poder leer bien los resultados de las ejecuciones, vamos a configurar tanto el número máximo de columnas como el número máximo de filas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número máximo de filas a mostrar\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Número máximo de columnas a mostrar\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Descripción general del dataset\n",
    "Para poder conocer ciertas características relevantes del dataset, como el número de instancias (filas) y características (columnas) procederemos a usar diferentes funciones de Pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                ID  AreaRecreativaID\n",
      "count  5005.000000      5.005000e+03\n",
      "mean   2501.294505      1.530310e+06\n",
      "std    1443.397288      2.791697e+06\n",
      "min       1.000000      1.665700e+04\n",
      "25%    1252.000000      1.742100e+04\n",
      "50%    2502.000000      1.811800e+04\n",
      "75%    3751.000000      4.513510e+05\n",
      "max    5000.000000      9.536964e+06\n",
      "\n",
      "\n",
      "Número de filas:  5005\n",
      "Número de columnas:  5\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5005 entries, 0 to 5004\n",
      "Data columns (total 5 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   ID                5005 non-null   int64 \n",
      " 1   FECHA_REPORTE     5005 non-null   object\n",
      " 2   TIPO_INCIDENTE    5005 non-null   object\n",
      " 3   GRAVEDAD          5005 non-null   object\n",
      " 4   AreaRecreativaID  5005 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 195.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Descripción de parámetros generales -> count, mean, std, min, 25%, 50%, 75%, max\n",
    "print(df.describe())\n",
    "\n",
    "# Número de filas y columnas del dataset\n",
    "print(\"\\n\")\n",
    "print(\"Número de filas: \", df.shape[0])\n",
    "print(\"Número de columnas: \", df.shape[1])\n",
    "print(\"\\n\")\n",
    "\n",
    "# Para saber el tipo de variable de cada columna\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Observación inicial del dataset\n",
    "Vamos a mostrar 30 entradas para poder observar cómo es realmente por dentro el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ----------    10 primeras filas    ----------    \n",
      "\n",
      "\n",
      "   ID FECHA_REPORTE TIPO_INCIDENTE GRAVEDAD  AreaRecreativaID\n",
      "0   1    2024/06/13           Robo     Alta             17538\n",
      "1   2    07-16-2024          Cáídá     Baja             16724\n",
      "2   3    2021/03/09          Caída  Crítica             18191\n",
      "3   4    10-15-2023      Accidente    Media             52162\n",
      "4   5    2022/06/25      Accidente     Alta             17397\n",
      "5   6    08/06/2020           Robo     Baja             17260\n",
      "6   7    2023/08/23     Vándálísmó  Crítica           7928031\n",
      "7   8    2022-01-15      Accidente  Crítica             17278\n",
      "8   9    27-08-2024     Vandalismo    Media             16729\n",
      "9  10    04-11-2020      Accídénté  Crítica             17203\n",
      "\n",
      "\n",
      "   ----------    10 filas aleatorias    ----------    \n",
      "\n",
      "\n",
      "        ID FECHA_REPORTE    TIPO_INCIDENTE GRAVEDAD  AreaRecreativaID\n",
      "3327  3328    2022-10-12              Robo     Baja             18316\n",
      "4668  4669    12/02/2022        Vandalismo     Baja             17610\n",
      "3472  3473    2023/08/19             Caída     Alta             17311\n",
      "2518  2519    2020/02/21              Róbó  Crítica            461183\n",
      "674    675    2024-01-08  Dáñó éstrúctúrál     Baja             52190\n",
      "3991  3992    18/02/2022         Accidente     Alta             16907\n",
      "4474  4475    18-06-2024  Daño estructural  Crítica             17135\n",
      "2849  2850    2024/07/05             Caída     Baja             17892\n",
      "2014  2015    2024/05/04              Robo     Alta             18027\n",
      "798    799    24-10-2019         Accídénté    Media           7127772\n",
      "\n",
      "\n",
      "   ----------    10 últimas filas    ----------    \n",
      "\n",
      "\n",
      "        ID FECHA_REPORTE    TIPO_INCIDENTE GRAVEDAD  AreaRecreativaID\n",
      "4995  4996    2024-01-22             Caída     Baja           3573018\n",
      "4996  4997    2020/03/01         Accidente    Media             16844\n",
      "4997  4998    2023/04/30         Accidente  Crítica             18207\n",
      "4998  4999    12-09-2022         Accidente  Crítica             17335\n",
      "4999  5000    06-03-2021  Dáñó éstrúctúrál    Media             16937\n",
      "5000  3501    07/07/2020        Vandalismo    Media           7125670\n",
      "5001  4346    27/08/2020        Vándálísmó     Baja             17297\n",
      "5002  3958    16/03/2020             Caída     Baja             18567\n",
      "5003  3353    19-08-2020             Cáídá     Baja             18590\n",
      "5004  1321    2023/12/31         Accidente     Baja             17931\n"
     ]
    }
   ],
   "source": [
    "# Mostramos las 10 primeras filas, 10 filas aleatorias y las 10 últimas\n",
    "print(\"   ----------    10 primeras filas    ----------    \")\n",
    "print(\"\\n\")\n",
    "print(df.head(10))\n",
    "print(\"\\n\")\n",
    "print(\"   ----------    10 filas aleatorias    ----------    \")\n",
    "print(\"\\n\")\n",
    "print(df.sample(10))\n",
    "print(\"\\n\")\n",
    "print(\"   ----------    10 últimas filas    ----------    \")\n",
    "print(\"\\n\")\n",
    "print(df.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Revisión de valores nulos\n",
    "Como ya se ha visto en el anterior apartado (info), podemos observar los valores nulos de esa forma. Pero se puede observar de una forma más visual en esta sección y, además, hay que tener en cuenta valores como el cero que también pueden considerarse nulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ----------    Valores faltantes    ----------    \n",
      "\n",
      "\n",
      "ID                  0\n",
      "FECHA_REPORTE       0\n",
      "TIPO_INCIDENTE      0\n",
      "GRAVEDAD            0\n",
      "AreaRecreativaID    0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "    ----------    Valores cero    ----------    \n",
      "\n",
      "\n",
      "ID                  0\n",
      "FECHA_REPORTE       0\n",
      "TIPO_INCIDENTE      0\n",
      "GRAVEDAD            0\n",
      "AreaRecreativaID    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Para poder saber el número de valores faltantes\n",
    "print(\"    ----------    Valores faltantes    ----------    \")\n",
    "print(\"\\n\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Para poder saber el número de ceros en cada columna\n",
    "print(\"    ----------    Valores cero    ----------    \")\n",
    "print(\"\\n\")\n",
    "print((df == 0).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No encontramos valores nulos, por lo que no se hará limpieza de esto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Identificación de fechas no estandarizadas\n",
    "Se deben identificar las fechas que no se encuentran en el formato adecuado para MongoDB (DD/MM/YYYY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ----------    Fechas    ----------    \n",
      "\n",
      "\n",
      "4537    03-10-2020\n",
      "2077    2022-09-03\n",
      "311     2020-11-29\n",
      "2910    2022/05/03\n",
      "2125    2023/05/23\n",
      "1987    09-01-2021\n",
      "1071    08-10-2023\n",
      "3429    07-14-2022\n",
      "1021    03-23-2020\n",
      "4967    2023-05-11\n",
      "3809    2022-11-22\n",
      "4068    04-19-2020\n",
      "2497    07-03-2020\n",
      "3450    2023-03-26\n",
      "643     27/01/2024\n",
      "843     18-07-2023\n",
      "507     2020-09-26\n",
      "781     01-18-2024\n",
      "2888    21-11-2019\n",
      "2316    2021/10/25\n",
      "Name: FECHA_REPORTE, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Vamos a mostrar algunas fechas para poder observar en qué formato están\n",
    "print(\"   ----------    Fechas    ----------    \")\n",
    "print(\"\\n\")\n",
    "print(df['FECHA_REPORTE'].sample(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede apreciar, las fechas se encuentran en diversos formatos que deben ser homogeneizados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Identificación de registros duplicados\n",
    "Debemos validar que no existen filas iguales que ensucien el dataset, sobre todo estando pendiente de duplicaciones de la clave primaria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ----------    Filas duplicadas    ----------    \n",
      "\n",
      "\n",
      "        ID FECHA_REPORTE TIPO_INCIDENTE GRAVEDAD  AreaRecreativaID\n",
      "5000  3501    07/07/2020     Vandalismo    Media           7125670\n",
      "5001  4346    27/08/2020     Vándálísmó     Baja             17297\n",
      "5002  3958    16/03/2020          Caída     Baja             18567\n",
      "5003  3353    19-08-2020          Cáídá     Baja             18590\n",
      "5004  1321    2023/12/31      Accidente     Baja             17931\n",
      "\n",
      "\n",
      "Número de filas duplicadas:  5\n",
      "\n",
      "\n",
      "   ----------    Filas con el mismo ID    ----------    \n",
      "\n",
      "\n",
      "        ID FECHA_REPORTE TIPO_INCIDENTE GRAVEDAD  AreaRecreativaID\n",
      "1320  1321    2023/12/31      Accidente     Baja             17931\n",
      "5004  1321    2023/12/31      Accidente     Baja             17931\n",
      "5003  3353    19-08-2020          Cáídá     Baja             18590\n",
      "3352  3353    19-08-2020          Cáídá     Baja             18590\n",
      "3500  3501    07/07/2020     Vandalismo    Media           7125670\n",
      "5000  3501    07/07/2020     Vandalismo    Media           7125670\n",
      "3957  3958    16/03/2020          Caída     Baja             18567\n",
      "5002  3958    16/03/2020          Caída     Baja             18567\n",
      "5001  4346    27/08/2020     Vándálísmó     Baja             17297\n",
      "4345  4346    27/08/2020     Vándálísmó     Baja             17297\n",
      "\n",
      "\n",
      "Número de filas con el mismo ID:  10\n"
     ]
    }
   ],
   "source": [
    "# Ver las filas duplicadas\n",
    "print(\"   ----------    Filas duplicadas    ----------    \")\n",
    "print(\"\\n\")\n",
    "print(df[df.duplicated()])\n",
    "print(\"\\n\")\n",
    "\n",
    "# Número de filas duplicadas\n",
    "print(\"Número de filas duplicadas: \", df.duplicated().sum())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Filtrar filas que tienen el mismo ID\n",
    "duplicados = df.groupby('ID').filter(lambda x: len(x) > 1)\n",
    "\n",
    "# Ordenar por NIF para que las filas con el mismo NIF se visualicen una encima de la otra\n",
    "duplicados = duplicados.sort_values(by='ID')\n",
    "\n",
    "# Mostrar las filas con la misma PK\n",
    "print(\"   ----------    Filas con el mismo ID    ----------    \")\n",
    "print(\"\\n\")\n",
    "print(duplicados)\n",
    "print(\"\\n\")\n",
    "print(\"Número de filas con el mismo ID: \", duplicados.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existen filas iguales que no difieren en nada entre ellas, por lo que pueden ser eliminadas sin problemas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 Búsqueda de errores tipográficos\n",
    "Hay ciertos atributos de texto que pueden contar con determinados errores tipográficos que deben ser solucionados, como los nombres de áreas, juegos, usuarios y ubicaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este dataset no encontramos ningún atributo similar, por lo que este paso no se realizará"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.9 Identificación de valores enum fuera de campo\n",
    "Hay ciertos atributos que solo deben poseer ciertos valores (como Operativo-NoOperativo). Hace falta identificar aquellos valores de ese campo fuera de la norma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ----------    Valores distintos de la columna 'TIPO_INCIDENTE'    ----------    \n",
      "\n",
      "\n",
      "['Robo' 'Cáídá' 'Caída' 'Accidente' 'Vándálísmó' 'Vandalismo' 'Accídénté'\n",
      " 'Daño estructural' 'Dáñó éstrúctúrál' 'Róbó']\n",
      "\n",
      "\n",
      "   ----------    Valores distintos de la columna 'GRAVEDAD'    ----------    \n",
      "\n",
      "\n",
      "['Alta' 'Baja' 'Crítica' 'Media']\n"
     ]
    }
   ],
   "source": [
    "# Muestra todos los valores distintos de la columna 'TIPO_INCIDENTE'\n",
    "print(\"   ----------    Valores distintos de la columna 'TIPO_INCIDENTE'    ----------    \")\n",
    "print(\"\\n\")\n",
    "print(df['TIPO_INCIDENTE'].unique())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Muestra todos los valores distintos de la columna 'GRAVEDAD'\n",
    "print(\"   ----------    Valores distintos de la columna 'GRAVEDAD'    ----------    \")\n",
    "print(\"\\n\")\n",
    "print(df['GRAVEDAD'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todos los Enums poseen valores acordes con lo esperado, por lo que no se va a realizar una limpieza como tal de ellos. Aunque se debe limpiar los valores de TIPO_INCIDENTE que poseen tildes raras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.10 Validación de las coordenadas y otros campos geoespaciales\n",
    "Hay algunas veces en las que los códigos postales no respetan la identificación de Madrid (280..) o el formato, ya sean códigos postales u otros atributos de geolocalización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este dataset no encontramos valores de localización, por lo que este paso no se realizará"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.11 Identificación de unidades de medida en un formato no estandarizado\n",
    "Se deben identificar las filas que no posean un formato estándar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este dataset no encontramos valores de unidades de medida, por lo que este paso no se realizará"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.12 Otros atributos a corregir\n",
    "En esta sección se mencionarán aquellos atributos que también deban ser limpiados por errores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.12.1 Validación de IDs\n",
    "Para este paso debemos comprobar que todos los IDs son enteros, ya que en el paso de los duplicados ya hemos comprobado esto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ----------    Validación de IDs    ----------    \n",
      "\n",
      "\n",
      "El ID es un entero:  True\n"
     ]
    }
   ],
   "source": [
    "# Debemos validar que todos los IDs son únicos y enteros\n",
    "print(\"   ----------    Validación de IDs    ----------    \")\n",
    "print(\"\\n\")\n",
    "print('El ID es un entero: ', df['ID'].apply(lambda x: x.is_integer()).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todos los IDs son enteros, por lo que no se precisará su limpieza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.12.2 Consideraciones extras de Fechas\n",
    "Cabe recalcar que si el campo de fechas no posee un formato esperado, las funciones de limpieza darán error, por lo que no hace falta hacer ahora las comprobaciones de sus valores para determinar si son correctos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.12.3 Consideraciones extras de AreaRecreativaID\n",
    "Se ha observado que los IDs presentes en el dataset de juegos comparten formato con estos y que, en este dataset, no hay nulos, por lo que lo único que validaremos es que todos los datos de esta columna están presentes en el dataset de áreas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ----------    Comprobación de valores en 'AreaRecreativaID'    ----------    \n",
      "\n",
      "\n",
      "Todos los valores se encuentran en el otro dataset:  True\n"
     ]
    }
   ],
   "source": [
    "# Comprobar que todos los valores de la columna 'AreaRecreativaID' existen en la columna 'ID' del dataset 'AreasSucio.csv'\n",
    "areas = pd.read_csv('../AreasSucio.csv')\n",
    "print(\"   ----------    Comprobación de valores en 'AreaRecreativaID'    ----------    \")\n",
    "print(\"\\n\")\n",
    "print(\"Todos los valores se encuentran en el otro dataset: \", df['AreaRecreativaID'].isin(areas['ID']).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Limpieza de los datasets\n",
    "En este apartado se realizará la limpieza según la información obtenida en el análisis exploratorio de datos:\n",
    "- Se debe corregir las fechas y dejarlas en un formato homogéneo\n",
    "- Se deben solucionar las filas repetidas, eliminando una de las dos iguales\n",
    "- Se deben limpiar los errores tipográficos de TIPO_INCIDENTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Limpieza de FECHA_REPORTE\n",
    "Como en otros datasets, debemos dejar todas las fechas en formato apto para MongoDB (DD/MM/YYYY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ----------    Columna de fechas corregidas    ----------    \n",
      "\n",
      "\n",
      "264     17/11/2019\n",
      "1666    21/06/2021\n",
      "3288    05/04/2024\n",
      "1010    19/11/2021\n",
      "2287    17/02/2024\n",
      "137     04/10/2021\n",
      "1280    06/10/2020\n",
      "2428    22/09/2022\n",
      "1286    12/02/2022\n",
      "1188    04/08/2024\n",
      "4645    04/11/2022\n",
      "1742    04/12/2020\n",
      "2131    17/11/2021\n",
      "2151    12/01/2022\n",
      "2926    19/05/2024\n",
      "1310    20/05/2022\n",
      "4844    28/07/2023\n",
      "4387    17/01/2020\n",
      "4600    10/05/2022\n",
      "839     25/07/2020\n",
      "Name: FECHA_REPORTE, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Leemos toda la columna de fechas y almacenamos todas las fechas en una lista\n",
    "fechas = df['FECHA_REPORTE'].tolist()\n",
    "\n",
    "# Ahora, pasamos por todas las fechas y corregimos las fechas que están mal escritas\n",
    "fechas_corregidas = []\n",
    "año_al_final = False\n",
    "\n",
    "for fecha in fechas:\n",
    "    # Si fecha contiene un guion, hacer fecha.split('-') y si fecha contiene una barra, hacer fecha.split('/')\n",
    "    if '-' in fecha:\n",
    "        fecha_split = fecha.split('-')\n",
    "    elif '/' in fecha:\n",
    "        fecha_split = fecha.split('/')\n",
    "    # Condiciones para saber el día, mes y año\n",
    "    # Encontrar el año\n",
    "    if (int(fecha_split[0])>31):\n",
    "        año = fecha_split[0]\n",
    "        año_al_final = False\n",
    "    elif (int(fecha_split[2])>31):\n",
    "        año = fecha_split[2]\n",
    "        año_al_final = True\n",
    "    else:\n",
    "        print(\"No se ha encontrado el año\")\n",
    "        fechas_corregidas.append(\"fecha incorrecta\")\n",
    "        break\n",
    "    # Encontrar el día y el mes\n",
    "    # Si el año está al final, no comprobamos el primer caracter\n",
    "    if año_al_final:\n",
    "        if (int(fecha_split[0])<32 and int(fecha_split[0])>12):\n",
    "            dia = fecha_split[0]\n",
    "            mes = fecha_split[1]\n",
    "        elif (int(fecha_split[1])<32 and int(fecha_split[1])>12):\n",
    "            dia = fecha_split[1]\n",
    "            mes = fecha_split[0]\n",
    "        # Si no hay ningun número entre el 13 y el 31, se asume que el mes es el segundo siempre y el día es el primero\n",
    "        else:\n",
    "            dia = fecha_split[0]\n",
    "            mes = fecha_split[1]\n",
    "    else:\n",
    "        if (int(fecha_split[1])<32 and int(fecha_split[1])>12):\n",
    "            dia = fecha_split[1]\n",
    "            mes = fecha_split[2]\n",
    "        elif (int(fecha_split[2])<32 and int(fecha_split[2])>12):\n",
    "            dia = fecha_split[2]\n",
    "            mes = fecha_split[1]\n",
    "        # Si no hay ningun número entre el 13 y el 31, se asume que el mes es el segundo siempre y el día es el tercero\n",
    "        else:\n",
    "            dia = fecha_split[2]\n",
    "            mes = fecha_split[1]\n",
    "    fechas_corregidas.append(dia+\"/\"+mes+\"/\"+año)\n",
    "    \n",
    "# Ahora, cambiamos toda la columna de FECHAS_INTERVENCION por las fechas corregidas\n",
    "df['FECHA_REPORTE'] = fechas_corregidas\n",
    "\n",
    "# Mostramos las fechas corregidas\n",
    "print(\"   ----------    Columna de fechas corregidas    ----------    \")\n",
    "print(\"\\n\")\n",
    "print(df['FECHA_REPORTE'].sample(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Limpieza de filas repetidas\n",
    "Se deben limpiar las filas iguales, eliminando una de las dos repetidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ----------    Filas con el mismo ID    ----------    \n",
      "\n",
      "\n",
      "        ID FECHA_REPORTE TIPO_INCIDENTE GRAVEDAD  AreaRecreativaID\n",
      "1320  1321    31/12/2023      Accidente     Baja             17931\n",
      "5004  1321    31/12/2023      Accidente     Baja             17931\n",
      "5003  3353    19/08/2020          Cáídá     Baja             18590\n",
      "3352  3353    19/08/2020          Cáídá     Baja             18590\n",
      "3500  3501    07/07/2020     Vandalismo    Media           7125670\n",
      "5000  3501    07/07/2020     Vandalismo    Media           7125670\n",
      "3957  3958    16/03/2020          Caída     Baja             18567\n",
      "5002  3958    16/03/2020          Caída     Baja             18567\n",
      "5001  4346    27/08/2020     Vándálísmó     Baja             17297\n",
      "4345  4346    27/08/2020     Vándálísmó     Baja             17297\n",
      "\n",
      "\n",
      "Número de filas con el mismo ID:  10\n",
      "\n",
      "\n",
      "   ----------    Filas con el mismo ID después de limpiar    ----------    \n",
      "\n",
      "\n",
      "Empty DataFrame\n",
      "Columns: [ID, FECHA_REPORTE, TIPO_INCIDENTE, GRAVEDAD, AreaRecreativaID]\n",
      "Index: []\n",
      "\n",
      "\n",
      "Número de filas duplicadas:  0\n",
      "\n",
      "\n",
      "   ----------    2 filas de las que antes estaban duplicadas    ----------    \n",
      "\n",
      "\n",
      "        ID FECHA_REPORTE TIPO_INCIDENTE GRAVEDAD  AreaRecreativaID\n",
      "1320  1321    31/12/2023      Accidente     Baja             17931\n",
      "        ID FECHA_REPORTE TIPO_INCIDENTE GRAVEDAD  AreaRecreativaID\n",
      "3352  3353    19/08/2020          Cáídá     Baja             18590\n"
     ]
    }
   ],
   "source": [
    "# Filtrar filas que tienen el mismo ID\n",
    "duplicados = df.groupby('ID').filter(lambda x: len(x) > 1)\n",
    "\n",
    "# Ordenar por NIF para que las filas con el mismo ID se visualicen una encima de la otra\n",
    "duplicados = duplicados.sort_values(by='ID')\n",
    "\n",
    "# Mostrar las filas con la misma PK\n",
    "print(\"   ----------    Filas con el mismo ID    ----------    \")\n",
    "print(\"\\n\")\n",
    "print(duplicados)\n",
    "print(\"\\n\")\n",
    "print(\"Número de filas con el mismo ID: \", duplicados.shape[0])\n",
    "print(\"\\n\")\n",
    "\n",
    "# Limpiamos una de las dos filas con el mismo ID de duplicados\n",
    "df.drop_duplicates(subset='ID', keep='first', inplace=True)\n",
    "\n",
    "# Mostramos el resultado por pantalla\n",
    "print(\"   ----------    Filas con el mismo ID después de limpiar    ----------    \")\n",
    "print(\"\\n\")\n",
    "print(df[df.duplicated()])\n",
    "print(\"\\n\")\n",
    "# Número de filas duplicadas\n",
    "print(\"Número de filas duplicadas: \", df.duplicated().sum())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Mostramos, para comprobar, 2 filas de las que antes estaban duplicadas\n",
    "print(\"   ----------    2 filas de las que antes estaban duplicadas    ----------    \")\n",
    "print(\"\\n\")\n",
    "print(df.loc[df['ID'] == 1321])\n",
    "print(df.loc[df['ID'] == 3353])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Limpieza de TIPO_INCIDENTE\n",
    "En esta columna Enum, tenemos ciertos valores con tildes cuando no deberían poseerlas. Hace falta limpiarlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ----------    Valores distintos de la columna 'TIPO_INCIDENTE'    ----------    \n",
      "\n",
      "\n",
      "['Robo' 'Caída' 'Accidente' 'Vandalismo' 'Daño estructural']\n"
     ]
    }
   ],
   "source": [
    "# Limpiamos la columna de 'TIPO_INCIDENTE' para que no haya tildes en lugares raros\n",
    "df['TIPO_INCIDENTE'] = df['TIPO_INCIDENTE'].str.replace('Cáídá', 'Caída')\n",
    "df['TIPO_INCIDENTE'] = df['TIPO_INCIDENTE'].str.replace('Róbó', 'Robo')\n",
    "df['TIPO_INCIDENTE'] = df['TIPO_INCIDENTE'].str.replace('Accídénté', 'Accidente')\n",
    "df['TIPO_INCIDENTE'] = df['TIPO_INCIDENTE'].str.replace('Vándálísmó', 'Vandalismo')\n",
    "df['TIPO_INCIDENTE'] = df['TIPO_INCIDENTE'].str.replace('Dáñó éstrúctúrál', 'Daño estructural')\n",
    "\n",
    "# Muestra todos los valores distintos de la columna 'TIPO_INCIDENTE'\n",
    "print(\"   ----------    Valores distintos de la columna 'TIPO_INCIDENTE'    ----------    \")\n",
    "print(\"\\n\")\n",
    "print(df['TIPO_INCIDENTE'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Mostrar Dataset Limpio y guardar CSV\n",
    "Vamos a mostrar algunas filas del dataset limpio para validar que todo está OK y guardamos el Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ----------    10 filas aleatorias    ----------    \n",
      "\n",
      "\n",
      "        ID FECHA_REPORTE    TIPO_INCIDENTE GRAVEDAD  AreaRecreativaID\n",
      "1828  1829    28/05/2021             Caída    Media             17084\n",
      "1782  1783    19/06/2020        Vandalismo  Crítica             16942\n",
      "1846  1847    18/11/2020         Accidente  Crítica             17608\n",
      "1723  1724    29/04/2020        Vandalismo    Media             16867\n",
      "4984  4985    05/11/2020  Daño estructural     Alta             17521\n",
      "1283  1284    05/09/2020         Accidente    Media             17379\n",
      "1607  1608    03/02/2023         Accidente     Alta             17572\n",
      "2374  2375    19/01/2023         Accidente     Baja             18539\n",
      "1437  1438    11/03/2024             Caída  Crítica             18540\n",
      "1984  1985    12/06/2020              Robo  Crítica             52115\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dataset limpio guardado\n"
     ]
    }
   ],
   "source": [
    "# Enseñamos 20 filas aleatorias para ver cómo quedan\n",
    "print(\"   ----------    10 filas aleatorias    ----------    \")\n",
    "print(\"\\n\")\n",
    "print(df.sample(10))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Guardamos el dataset limpio\n",
    "df.to_csv('../IncidentesSeguridadLimpio.csv', index=False)\n",
    "print(\"\\n\")\n",
    "print(\"Dataset limpio guardado\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
